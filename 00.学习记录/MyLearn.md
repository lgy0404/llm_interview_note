# 学习记录

## 已完成的部分

* 240611

  * 10.1 langchain
* 20240613

  * 结合langchain例程进行学习
  * 待解决的问题：
    * jupyter notebook conda环境
    * api调用的时候代理设置
* 20240614

  * 添加langchain例程
  * 在vscode中运行
  * 换用deepseek模型，API和URL已设置环境变量
* 20240615

  * 切换回了GPT
  * 新建conda环境：langchain
  * 完成第一个langchain例程的学习
* 20240616

  * 完成langchain-memory的学习
* 20240617

  * 完成langchain-chain的学习
  * 待解决问题：API调用超时，一直没结果【240618更新：conda环境问题，用py39可以】
* 20240618

  * 5.Question and Answer看了一半
  * 中间向量库搭建报错代解决
* 20240620

  * langchain例程学习结束
* 20240621

  * 08.检索增强rag/大模型agent技术
* 20240622

  * 08.检索增强rag/rag（检索增强生成）技术
* 20240623

  * **langchain rag 环境配置报错待解决**
* 20240624

  * 01.大语言模型基础/1.语言模型/1.语言模型.md
* 20240625

  * 01.大语言模型基础/1.分词/1.分词.md
* 20240626

  * 到02.大语言模型架构/1.attention
* 20240627

  * 学习注意力机制代码  `code/259_注意力机制.ipynb`
  * 后边pytorch相关的代码统一存储到 `https://github.com/lgy0404/pytotch-learn`
* 20240628

  * **注意力机制代码  `code/259_注意力机制.ipynb` 没看懂先跳过**
* 20240629

  * `code/260_注意力分数.ipynb` 没看懂先跳过
* 20240630

  * 切换transformers库，d2l太老了！
  * transformers
    * [【手把手带你实战HuggingFace Transformers-入门篇】基础知识与环境安装](https://www.bilibili.com/video/BV1ma4y1g791/?spm_id_from=333.788&vd_source=65e0a21e65c5e67e051e0f63a24a7aeb)
  * pytorch看土堆的足矣
* 240701

  * 土堆pytorch代码精读：100-107
* 240702

  * 土堆pytorch代码精读：108-122，完结！
* 240703

  * transformer代码逐行精读资料整理
* 240704

  * 添加源码

    * https://github.com/wmathor/nlp-tutorial.git
  * 对应视频

    * https://www.bilibili.com/video/BV1Qg4y1P7r4/?spm_id_from=333.337.search-card.all.click&vd_source=65e0a21e65c5e67e051e0f63a24a7aeb
* 240708

  * 视频学习，感觉现在找的资料不是很可用
* 240709

  * 添加代码：https://github.com/lansinuote/Transformer_Example.git
  * 确定transformer学习视频：https://www.bilibili.com/video/BV19Y411b7qx?p=1&vd_source=65e0a21e65c5e67e051e0f63a24a7aeb
* 240710

  * 视频看到：12.代码讲解训练和测试
  * 源码需要静心看啊！

## TODO

* https://www.langchain.com.cn/use_cases/agent_simulations/camel_role_playing
*
